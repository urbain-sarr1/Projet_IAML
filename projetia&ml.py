# -*- coding: utf-8 -*-
"""ProjetIA&ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t12Mi2T_QhGKERx-G6GcyRhWdJjrsAQO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import make_scorer, accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Chargement des donn√©es clients
df = pd.read_csv('churn_clients.csv')

# Affichage des premi√®res lignes du dataframe
print("\nPremi√®res lignes du dataframe :\n")
print(df)

# V√©rification du nombre de clients
print("\nNombre de Client :", len(df))

# V√©rification des valeurs manquantes
print("\nValeurs manquantes dans le dataframe :\n")
print(df.isnull().sum())

# V√©rification des doublon
print("\nNombre de doublons :", df.duplicated().sum())

# Encodage des variables cat√©gorielles
label_encoder = LabelEncoder()
df['Sexe'] = label_encoder.fit_transform(df['Sexe'])
df['Support_contacte'] = label_encoder.fit_transform(df['Support_contacte'])

# Visualisations
plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], kde=True)
plt.title('Distribution de l\'√¢ge')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['Revenu'], kde=True)
plt.title('Distribution du revenu')
plt.show()

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Matrice de corr√©lation')
plt.show()

print("\nDonn√©es apr√®s encodage")
print(df.head())

# Normalisation des colonnes num√©riques
scaler = StandardScaler()
df[['Age', 'Revenu', 'Anciennete', 'Frequence_utilisation', 'Score_satisfaction']] = scaler.fit_transform(df[['Age', 'Revenu', 'Anciennete', 'Frequence_utilisation', 'Score_satisfaction']])

print("\nDonn√©es apr√®s standarnisation\n")
print(df.head())

# 1. S√©paration des donn√©es
X = df.drop('Resilie', axis=1)
y = df['Resilie']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 2. D√©finition des mod√®les avec r√©gularisation
models = {
    'R√©gression Logistique': LogisticRegression(penalty='l2', C=0.1, solver='liblinear', random_state=42),
    'Arbre de D√©cision': DecisionTreeClassifier(max_depth=3, min_samples_leaf=10, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=10, weights='distance')
}

# 3. Validation crois√©e avec Pipeline (scaling + SMOTE)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name, model in models.items():
    print(f"\nüîç Mod√®le : {model_name}")

    # Pipeline : standardisation -> SMOTE -> mod√®le
    pipeline = ImbPipeline([
        ('scaler', StandardScaler()),
        ('smote', SMOTE(random_state=42)),
        ('model', model)
    ])

    # Cross-val avec 4 scores
    for metric, scorer in {
        'Accuracy': make_scorer(accuracy_score),
        'Recall': make_scorer(recall_score),
        'F1 Score': make_scorer(f1_score),
        'AUC': 'roc_auc'
    }.items():
        cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)
        print(f"{metric} (cross-val) : {cv_scores.mean():.4f}")

# 4. √âvaluation sur le test set
    # Pr√©paration : scaling + SMOTE uniquement sur le train
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    sm = SMOTE(random_state=42)
    X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)

    # Entra√Ænement
    model.fit(X_train_res, y_train_res)
    y_pred = model.predict(X_test_scaled)

    # √âvaluation finale
    accuracy = accuracy_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"\n--- √âvaluation sur le test set ---")
    print(f"Pr√©cision : {accuracy:.4f}")
    print(f"Rappel : {recall:.4f}")
    print(f"F1 Score : {f1:.4f}")
    print(f"AUC : {auc:.4f}")
    print(f"Matrice de confusion :\n{conf_matrix}")

    # Courbe ROC
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test_scaled)[:, 1])
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

# 5. Finalisation de la courbe ROC
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('Taux de faux positifs')
plt.ylabel('Taux de vrais positifs')
plt.title('Courbe ROC - Test Set')
plt.legend(loc='best')
plt.grid(True)
plt.show()

print("üìä Importance des variables par mod√®le")

# On reprend les noms de colonnes d'origine
features = X.columns

for model_name, model in models.items():
    print(f"\nüîç Mod√®le : {model_name}")

    if model_name == 'R√©gression Logistique':
        # On r√©cup√®re les coefficients apr√®s entra√Ænement sur donn√©es standardis√©es et SMOTE
        importance = model.coef_[0]
        importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})
        importance_df = importance_df.reindex(importance_df.Importance.abs().sort_values(ascending=False).index)

        for _, row in importance_df.iterrows():
            print(f"{row['Feature']} : {row['Importance']:.4f}")

        plt.figure(figsize=(8, 5))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title(f"Importance des variables - {model_name}")
        plt.xlabel("Poids (coefficient)")
        plt.ylabel("Variables")
        plt.tight_layout()
        plt.show()

    elif model_name == 'Arbre de D√©cision':
        importance = model.feature_importances_
        importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})
        importance_df = importance_df.sort_values(by='Importance', ascending=False)

        for _, row in importance_df.iterrows():
            print(f"{row['Feature']} : {row['Importance']:.4f}")

        plt.figure(figsize=(8, 5))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title(f"Importance des variables - {model_name}")
        plt.xlabel("Importance")
        plt.ylabel("Variables")
        plt.tight_layout()
        plt.show()

    elif model_name == 'KNN':
        print("‚ÑπÔ∏è Le mod√®le KNN ne donne pas directement l‚Äôimportance des variables.")
        # On applique permutation importance ici (sur le mod√®le d√©j√† entra√Æn√©)
        result = permutation_importance(model, X_test_scaled, y_test, n_repeats=30, random_state=42, scoring='accuracy')

        perm_importance_df = pd.DataFrame({
            'Feature': features,
            'Importance': result.importances_mean
        }).sort_values(by='Importance', ascending=False)

        print("\nüîé Importance des variables (Permutation) - KNN :")
        print(perm_importance_df)

        plt.figure(figsize=(8, 5))
        sns.barplot(x='Importance', y='Feature', data=perm_importance_df)
        plt.title("Importance des variables - KNN (Permutation)")
        plt.xlabel("Importance moyenne (baisse d'accuracy)")
        plt.ylabel("Variables")
        plt.tight_layout()
        plt.show()

# Variables √† garder selon l'importance
variables_utiles = ['Anciennete', 'Frequence_utilisation', 'Score_satisfaction']

# Nouveau X r√©duit
X_reduit = df[variables_utiles]
y = df['Resilie']

# 1. Split
X_train, X_test, y_train, y_test = train_test_split(X_reduit, y, test_size=0.2, stratify=y, random_state=42)

# 2. Nouveau mod√®le arbre
arbre_simplifie = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10, random_state=42)

# 3. Pipeline (scaling + SMOTE + mod√®le)
pipeline = ImbPipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('model', arbre_simplifie)
])

# 4. Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for metric, scorer in {
    'Accuracy': make_scorer(accuracy_score),
    'Recall': make_scorer(recall_score),
    'F1 Score': make_scorer(f1_score),
    'AUC': 'roc_auc'
}.items():
    scores = cross_val_score(pipeline, X_reduit, y, cv=cv, scoring=scorer)
    print(f"{metric} (cross-val) : {scores.mean():.4f}")

# 5. R√©entra√Æner sur train + SMOTE
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)

arbre_simplifie.fit(X_train_res, y_train_res)
y_pred = arbre_simplifie.predict(X_test_scaled)

# 6. √âvaluer
print("\n--- √âvaluation sur le test set ---")
print(f"Pr√©cision : {accuracy_score(y_test, y_pred):.4f}")
print(f"Rappel : {recall_score(y_test, y_pred):.4f}")
print(f"F1 Score : {f1_score(y_test, y_pred):.4f}")
print(f"AUC : {roc_auc_score(y_test, arbre_simplifie.predict_proba(X_test_scaled)[:, 1]):.4f}")
print(f"Matrice de confusion :\n{confusion_matrix(y_test, y_pred)}")

# 7. Courbe ROC
fpr, tpr, _ = roc_curve(y_test, y_pred)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f'Arbre simplifi√© (AUC = {roc_auc_score(y_test, y_pred):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('Taux de faux positifs')
plt.ylabel('Taux de vrais positifs')
plt.title('Courbe ROC - Arbre simplifi√©')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 8. Affichage de l'importance des variables
importances = arbre_simplifie.feature_importances_
importance_df = pd.DataFrame({
    'Feature': variables_utiles,
    'Importance': importances
})

# Trier par importance d√©croissante
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Afficher les importances des variables
print("\nImportance des variables :")
print(importance_df)

# Optionnel : Visualiser l'importance des variables
importance_df.plot(kind='bar', x='Feature', y='Importance', legend=False)
plt.title('Importance des variables')
plt.xlabel('Variables')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

import shap

# Cr√©er un DataFrame avec les noms des colonnes
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=variables_utiles)

# Cr√©er un explainer SHAP pour l'arbre de d√©cision
explainer = shap.TreeExplainer(arbre_simplifie)

# Obtenir les valeurs SHAP
shap_values = explainer.shap_values(X_test_scaled_df)

# S√©lectionner un utilisateur sp√©cifique (par exemple, l'Observation 1)
observation_idx = 0  # Choisis l'indice de l'observation souhait√©e (ici, l'utilisateur 1)

# Afficher la pr√©diction et l'impact des features pour cet utilisateur
prediction = arbre_simplifie.predict([X_test_scaled_df.iloc[observation_idx]])

# Afficher l'explication SHAP
print(f"\n--- Explication de la pr√©diction pour l'Observation {observation_idx + 1} ---")
print(f"Pr√©diction : {prediction[0]} (1 signifie r√©siliation, 0 signifie non r√©siliation)\n")

# Afficher les valeurs SHAP pour chaque feature (classe 1)
print(f"Valeurs SHAP pour chaque feature (classe 1) :")
for feature, shap_value in zip(variables_utiles, shap_values[1][observation_idx]):
    print(f"{feature}: {shap_value:.4f}")

# Calculer la pr√©diction attendue (base value) et l'impact total
expected_value = explainer.expected_value[1]
impact_total = expected_value + shap_values[1][observation_idx].sum()

print(f"\nValeur attendue (base value) : {expected_value:.4f}")
print(f"Impact total de l'utilisateur : {impact_total:.4f}")